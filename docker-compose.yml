volumes:
  ollama_storage:

networks:
  demo:

x-n8n: &service-n8n
  image: n8nio/n8n:latest
  networks: ['demo']
  deploy:
    resources:
      limits:
        memory: 0.5g
  environment:
    - N8N_DIAGNOSTICS_ENABLED=false
    - N8N_PERSONALIZATION_ENABLED=false
    - N8N_ENCRYPTION_KEY
    - N8N_USER_MANAGEMENT_JWT_SECRET
    - OLLAMA_HOST=${OLLAMA_HOST:-ollama:11434}
  env_file:
    - path: .env
      required: true

x-ollama: &service-ollama
  image: 7affer/bwilk:small-ollama-alpine
  container_name: ollama
  networks: ['demo']
  restart: unless-stopped
  ports:
    - 11434:80
  volumes:
    - ollama_storage:/root/.ollama

x-init-ollama: &init-ollama
  image: 7affer/bwilk:small-ollama-alpine
  networks: ['demo']
  container_name: ollama-pull-llama
  volumes:
    - ollama_storage:/root/.ollama
  entrypoint: /bin/sh
  environment:
    - OLLAMA_HOST=ollama:11434
  command:
    - "-c"
    # - "sleep 3; ollama pull llama3.2"
    - "sleep 3; ollama pull smollm:135m"

services:
  n8n:
    <<: *service-n8n
    hostname: n8n
    container_name: n8n
    restart: unless-stopped
    ports:
      - 80:5678
    volumes:
      - ./n8n/demo-data:/demo-data
      - ./shared:/data/shared

  ollama-cpu:
    <<: *service-ollama

  ollama-pull-llama-cpu:
    <<: *init-ollama
    depends_on:
      - ollama-cpu